The Google File System

1. A scalable distributed file system for large distributed data-intensive applications
2. Fault tolerance while running on inexpensive commodity hardware, and it delivers high aggregate performance to a large number of clients


Introduction:
1. Its design has been driven by key observations of our application workloads and technological environment, both current and anticipated, that reflect a marked departure from some earlier file system design assumptions.
2. Component failures are the norm rather than the exception, so constant monitoring, error detection, fault tolerance and automatic recovery must be integral to the system.
3. Files are huge by traditional standards.
4. Most files are mutated by appending new data rather than overwriting existing data.
5. Co-designing the applications and the file system API benefits the overall system by increasing our flexibility.


Assumptions:
1. The system is built from many inexpensive commodity components that often fail.
2. The system stores a modest number of large files.
3. The workloads primarily consist of two kinds of reads: large streaming reads and small random reads.
4. The workloads also have many large, sequential writes that append data to files.
5. The system must efficiently implement well-defined semantics for multiple clients that currently append to the same file.
6. High sustained bandwidth is more important than low latency.



Architecture:
1. A GFS cluster consists of 1 single master and multiple chunk servers and is accessed by multiple clients.
2. Files are divided into fixed-size chunks. Each chunk is identified by an immutable and globally unique 64 bit chunk handle assigned by he master at the time of chunk creation. By default, each chunk is replicated on 3 chunk servers.
3. The master maintains all file system metadata.
    a. It includes, the namespace, access control information, the mapping from files to chunks, and the current locations of chunks.
    b. Controls system-wide activities suck as chunk lease management, garbage collections of orphaned chunks, and chunk migration between chunk servers.
    c. Periodically communicates with each chunk server in HeartBeat messages
4. Neither client nor the chunk server caches file data, b/c chunks are stored as local files so Linux's buffer cache already keeps frequently accessed data in memory.



Single Master
1. Clients never read and write file data through the master. Instead, a client asks the master which chunk servers it should contact. It caches this info for a limited time and interacts with the chunk servers directly for many subsequent operations.


Chunk Size
1. 64MB
2. Each chunk replica is stored as a plain Linux file on a chunk server and is extended only as needed.
3. Advantage :
    a. Reduce clients' need to interact with master, and can easily and comfortably cache all the chunk location information for a multi-TB working set.
    b. Reduce network overhead by keeping a consistent TCP connection to he chunk server over an extended period of time.
    c. Reduces the size of the metadata on the master.


Metadata
1. 3 major types : the file and chunk namespaces, the mapping from files to chunks, and the locations of each chunk's replicas.
2. ALL are in memory. First 2 are stored on master's local disk persistently. The latter one is stored and acquired at master startup, whenever a chunk server joins the cluster, and HeartBeat scanning.